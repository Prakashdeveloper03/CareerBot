{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoQ4_TlXybJ9"
   },
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mVkBeAG-dTPF"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-iJx8HwyhLr"
   },
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RsJ7lLK1Cay_"
   },
   "outputs": [],
   "source": [
    "with open(\"../datasmalltalk.txt\", \"r\") as file:\n",
    "    raw_data = [line.split(\"\\t\") for line in file.read().split(\"\\n\")]\n",
    "raw_data_np = np.array(raw_data)\n",
    "questions = raw_data_np[:, 0]\n",
    "answers = np.where(raw_data_np[:, 1] != \"\", raw_data_np[:, 1], \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeqszYkR3v7F"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFuAVvFuyo64"
   },
   "source": [
    "### Data Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WXM48uqPI24W"
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Any, Union\n",
    "\n",
    "\n",
    "def tokenize(\n",
    "    lang: List[str],\n",
    "    max_vocab_size: Optional[int] = None,\n",
    "    max_seq_length: Optional[int] = None,\n",
    "    padding_type: str = \"post\",\n",
    ") -> Tuple[np.ndarray, tf.keras.preprocessing.text.Tokenizer]:\n",
    "    \"\"\"\n",
    "    Tokenizes a list of sentences and pads sequences to a maximum length.\n",
    "\n",
    "    Args:\n",
    "        lang (list): A list of sentences to be tokenized.\n",
    "        max_vocab_size (int, optional): Maximum vocabulary size. Words outside the vocabulary will be ignored. Defaults to None.\n",
    "        max_seq_length (int, optional): Maximum sequence length. Sequences longer than this will be truncated, and sequences shorter will be padded. Defaults to None.\n",
    "        padding_type (str, optional): Type of padding. Can be 'pre' or 'post'. Defaults to 'post'.\n",
    "\n",
    "    Returns:\n",
    "        tensor (numpy.ndarray): A 2D numpy array representing the tokenized and padded sequences.\n",
    "        lang_tokenizer (tf.keras.preprocessing.text.Tokenizer): A Tokenizer object fitted on the input language.\n",
    "    \"\"\"\n",
    "    # Initialize a Tokenizer object\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=max_vocab_size, filters=\"\"\n",
    "    )\n",
    "\n",
    "    # Fit the Tokenizer on the input language\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    # Convert sentences to sequences of tokens\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    # Pad sequences to a maximum length if provided\n",
    "    if max_seq_length is not None:\n",
    "        tensor = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            tensor, padding=padding_type, maxlen=max_seq_length\n",
    "        )\n",
    "    else:\n",
    "        tensor = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            tensor, padding=padding_type\n",
    "        )\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P57M5HwNyrtM"
   },
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qtQWA702Iwxx"
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesses a single sentence by removing diacritics, converting to lowercase,\n",
    "    adding start and end tokens, and cleaning punctuation.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): Input sentence to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        str: Preprocessed sentence.\n",
    "    \"\"\"\n",
    "    # Remove diacritics\n",
    "    sentence = \"\".join(\n",
    "        c\n",
    "        for c in unicodedata.normalize(\"NFD\", sentence)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "    # Convert to lowercase and strip leading/trailing whitespaces\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    # Add space around punctuation marks\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    # Remove any characters that are not letters, punctuation marks, or spaces\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "\n",
    "    # Add start and end tokens\n",
    "    sentence = \"<start> \" + sentence + \" <end>\"\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ubN0jC6zJ0hH"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "\n",
    "def preprocess_sentences(sentences: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Preprocesses a list of sentences in parallel using multiprocessing.\n",
    "\n",
    "    Args:\n",
    "        sentences (list): List of sentences to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        list: Preprocessed sentences.\n",
    "    \"\"\"\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        preprocessed_sentences = pool.map(preprocess_sentence, sentences)\n",
    "    return preprocessed_sentences\n",
    "\n",
    "\n",
    "# Preprocess questions and answers using multiprocessing\n",
    "pre_questions = preprocess_sentences(questions)\n",
    "pre_answers = preprocess_sentences(answers)\n",
    "data = pre_answers, pre_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ryMnanp0KBWw"
   },
   "outputs": [],
   "source": [
    "def prepare_data(\n",
    "    data: Tuple[list, list],\n",
    "    max_vocab_size: Optional[int] = None,\n",
    "    max_seq_length: Optional[int] = None,\n",
    ") -> Tuple[\n",
    "    np.ndarray,\n",
    "    np.ndarray,\n",
    "    tf.keras.preprocessing.text.Tokenizer,\n",
    "    tf.keras.preprocessing.text.Tokenizer,\n",
    "]:\n",
    "    \"\"\"\n",
    "    Prepares data for training by tokenizing input and target languages.\n",
    "\n",
    "    Args:\n",
    "        data (Tuple[list, list]): A tuple containing target and input language data.\n",
    "        max_vocab_size (Optional[int], optional): Maximum vocabulary size for tokenization. Defaults to None.\n",
    "        max_seq_length (Optional[int], optional): Maximum sequence length for padding. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, tf.keras.preprocessing.text.Tokenizer, tf.keras.preprocessing.text.Tokenizer]: A tuple containing input tensor, target tensor, input language tokenizer, and target language tokenizer.\n",
    "    \"\"\"\n",
    "    targ_lang, inp_lang = data\n",
    "\n",
    "    # Tokenize input and target languages\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(\n",
    "        inp_lang, max_vocab_size, max_seq_length\n",
    "    )\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(\n",
    "        targ_lang, max_vocab_size, max_seq_length\n",
    "    )\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJV_dqtyKYzm",
    "outputId": "523265ae-f056-4d7f-b762-e075dffaf745"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "input_tensor, target_tensor, inp_lang, targ_lang = prepare_data(data)\n",
    "max_length_targ = target_tensor.shape[1]\n",
    "max_length_inp = input_tensor.shape[1]\n",
    "print(max_length_targ)\n",
    "print(max_length_inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7uEiPyo7DyI"
   },
   "source": [
    "### Download Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Fjor8_8K6__v"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def save_tokenizer(tokenizer, filename):\n",
    "    with open(filename, \"wb\") as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "save_tokenizer(inp_lang, \"input_tokenizer.pkl\")\n",
    "save_tokenizer(targ_lang, \"target_tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqptaUlQyw5m"
   },
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bXEJ-kkDWDIH"
   },
   "outputs": [],
   "source": [
    "def save_tokenizer(tokenizer: Any, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves a tokenizer object to a file using pickle serialization.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (Any): Tokenizer object to be saved.\n",
    "        filename (str): Name of the file to save the tokenizer to.\n",
    "    \"\"\"\n",
    "    with open(filename, \"wb\") as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_tokenizer(filename: str) -> Any:\n",
    "    \"\"\"\n",
    "    Loads a tokenizer object from a file using pickle deserialization.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Name of the file containing the tokenizer object.\n",
    "\n",
    "    Returns:\n",
    "        Any: Loaded tokenizer object.\n",
    "    \"\"\"\n",
    "    with open(filename, \"rb\") as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "save_tokenizer(inp_lang, \"input_tokenizer.pkl\")\n",
    "save_tokenizer(targ_lang, \"target_tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uP0JQLrPrOWT"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into 90% train, 10% validation\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = (\n",
    "    train_test_split(input_tensor, target_tensor, test_size=0.1, random_state=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1mCYuBF4Crv"
   },
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3nQQ917yzKF"
   },
   "source": [
    "### Defining the PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUeolMd9SKHY",
    "outputId": "b282a005-982b-4a1d-bdc0-f2abe57e4051"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 24]), TensorShape([64, 24]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train) // BATCH_SIZE\n",
    "embedding_dim = 400\n",
    "units = 1500\n",
    "vocab_inp_size = len(inp_lang.word_index) + 1\n",
    "vocab_tar_size = len(targ_lang.word_index) + 1\n",
    "\n",
    "\n",
    "def create_dataset(\n",
    "    input_tensor: tf.Tensor, target_tensor: tf.Tensor, batch_size: int\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Creates a TensorFlow dataset from input and target tensors.\n",
    "\n",
    "    Args:\n",
    "        input_tensor (tf.Tensor): Tensor containing input sequences.\n",
    "        target_tensor (tf.Tensor): Tensor containing target sequences.\n",
    "        batch_size (int): Batch size for the dataset.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: TensorFlow dataset containing input-target pairs.\n",
    "    \"\"\"\n",
    "    # Determine buffer size for shuffling\n",
    "    buffer_size = len(input_tensor)\n",
    "\n",
    "    # Create dataset from tensors and shuffle\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor)).shuffle(\n",
    "        buffer_size\n",
    "    )\n",
    "\n",
    "    # Batch the dataset\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataset = create_dataset(input_tensor_train, target_tensor_train, BATCH_SIZE)\n",
    "\n",
    "# Example input and target batches\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape\n",
    "\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Nk-cafrc3WM8"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Encoder class for sequence-to-sequence models.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "        embedding_dim (int): Dimensionality of the embedding space.\n",
    "        enc_units (int): Number of units in the encoder LSTM layer.\n",
    "        batch_sz (int): Batch size.\n",
    "\n",
    "    Attributes:\n",
    "        batch_sz (int): Batch size.\n",
    "        enc_units (int): Number of units in the encoder LSTM layer.\n",
    "        embedding (tf.keras.layers.Embedding): Embedding layer.\n",
    "        lstm (tf.keras.layers.LSTM): LSTM layer.\n",
    "\n",
    "    Methods:\n",
    "        call(x, hidden):\n",
    "            Performs the forward pass of the encoder.\n",
    "        initialize_hidden_state():\n",
    "            Initializes the hidden state of the encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, vocab_size: int, embedding_dim: int, enc_units: int, batch_sz: int\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            self.enc_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer=\"glorot_uniform\",\n",
    "        )\n",
    "\n",
    "    def call(self, x: tf.Tensor, hidden: tf.Tensor) -> Union[tf.Tensor, list]:\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the encoder.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): Input tensor of shape (batch_size, sequence_length).\n",
    "            hidden (tf.Tensor): Initial hidden state of the LSTM.\n",
    "\n",
    "        Returns:\n",
    "            output (tf.Tensor): Output tensor of shape (batch_size, sequence_length, enc_units).\n",
    "            state (list): List containing the final hidden states.\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        output, state_h, state_c = self.lstm(x, initial_state=hidden)\n",
    "        state = [state_h, state_c]\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self) -> list:\n",
    "        \"\"\"\n",
    "        Initializes the hidden state of the encoder.\n",
    "\n",
    "        Returns:\n",
    "            list: List containing the initial hidden states.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            tf.zeros((self.batch_sz, self.enc_units)),\n",
    "            tf.zeros((self.batch_sz, self.enc_units)),\n",
    "        ]\n",
    "\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "adO6-QxYKlwx"
   },
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom attention layer for sequence-to-sequence models.\n",
    "\n",
    "    Args:\n",
    "        units (int): Number of units for the attention mechanism.\n",
    "\n",
    "    Attributes:\n",
    "        W1 (tf.keras.layers.Dense): Dense layer for query transformation.\n",
    "        W2 (tf.keras.layers.Dense): Dense layer for values transformation.\n",
    "        V (tf.keras.layers.Dense): Dense layer for attention score computation.\n",
    "\n",
    "    Methods:\n",
    "        call(query, values):\n",
    "            Computes the attention weights and context vector.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units: int):\n",
    "        \"\"\"\n",
    "        Initializes the attention layer.\n",
    "\n",
    "        Args:\n",
    "            units (int): Number of units for the attention mechanism.\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query: tf.Tensor, values: tf.Tensor) -> Union[tf.Tensor, tf.Tensor]:\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            query (tf.Tensor): Query tensor of shape (batch_size, query_length, hidden_size).\n",
    "            values (tf.Tensor): Values tensor of shape (batch_size, values_length, hidden_size).\n",
    "\n",
    "        Returns:\n",
    "            context_vector (tf.Tensor): Context vector of shape (batch_size, hidden_size).\n",
    "            attention_weights (tf.Tensor): Attention weights of shape (batch_size, values_length, 1).\n",
    "        \"\"\"\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ucM7JAxI4vM3"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Decoder class for sequence-to-sequence models with attention.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the target vocabulary.\n",
    "        embedding_dim (int): Dimensionality of the embedding space.\n",
    "        dec_units (int): Number of units in the decoder LSTM layer.\n",
    "        batch_sz (int): Batch size.\n",
    "\n",
    "    Attributes:\n",
    "        batch_sz (int): Batch size.\n",
    "        dec_units (int): Number of units in the decoder LSTM layer.\n",
    "        embedding (tf.keras.layers.Embedding): Embedding layer.\n",
    "        lstm (tf.keras.layers.LSTM): LSTM layer.\n",
    "        fc (tf.keras.layers.Dense): Fully connected layer.\n",
    "        attention (Attention): Attention mechanism.\n",
    "\n",
    "    Methods:\n",
    "        call(x, hidden, enc_output):\n",
    "            Performs the forward pass of the decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, vocab_size: int, embedding_dim: int, dec_units: int, batch_sz: int\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            self.dec_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer=\"glorot_uniform\",\n",
    "        )\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = Attention(self.dec_units)\n",
    "\n",
    "    def call(\n",
    "        self, x: tf.Tensor, hidden: list, enc_output: tf.Tensor\n",
    "    ) -> Union[tf.Tensor, list, tf.Tensor]:\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the decoder.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): Input tensor of shape (batch_size, 1).\n",
    "            hidden (list): List of initial hidden states.\n",
    "            enc_output (tf.Tensor): Encoder output tensor of shape (batch_size, sequence_length, hidden_size).\n",
    "\n",
    "        Returns:\n",
    "            x (tf.Tensor): Output tensor of shape (batch_size, vocab_size).\n",
    "            state (list): List of final hidden states.\n",
    "            attention_weights (tf.Tensor): Attention weights of shape (batch_size, sequence_length, 1).\n",
    "        \"\"\"\n",
    "        context_vector, attention_weights = self.attention(hidden[0], enc_output)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state_h, state_c = self.lstm(x, initial_state=hidden)\n",
    "        state = [state_h, state_c]\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights\n",
    "\n",
    "\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uHuaQpny3Wk"
   },
   "source": [
    "###Adjusting Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Z0Z62qj-ZLru"
   },
   "outputs": [],
   "source": [
    "# Define the initial learning rate for adaptive learning rate\n",
    "initial_learning_rate = 0.001\n",
    "\n",
    "# Define the learning rate schedule using Exponential Decay\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True,\n",
    ")\n",
    "\n",
    "# Define the optimizer with the adaptive learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Define the loss function\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "def loss_function(real: tf.Tensor, pred: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Custom loss function for sequence-to-sequence models.\n",
    "\n",
    "    Args:\n",
    "        real (tf.Tensor): Ground truth labels, shape (batch_size, sequence_length).\n",
    "        pred (tf.Tensor): Predicted logits, shape (batch_size, sequence_length, vocab_size).\n",
    "\n",
    "    Returns:\n",
    "        loss (tf.Tensor): Mean loss over the sequence, excluding padding tokens.\n",
    "    \"\"\"\n",
    "    # Create a mask to exclude padding tokens from loss calculation\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "\n",
    "    # Calculate the loss using Sparse Categorical Crossentropy\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Apply the mask to the loss\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    # Compute the mean loss over the sequence\n",
    "    loss = tf.reduce_mean(loss_)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXFR4CAxy9Od"
   },
   "source": [
    "###Defining Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ftqY-Rj_XNlW"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp: tf.Tensor, targ: tf.Tensor, enc_hidden: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Performs a single training step.\n",
    "\n",
    "    Args:\n",
    "        inp (tf.Tensor): Input sequences, shape (batch_size, input_sequence_length).\n",
    "        targ (tf.Tensor): Target sequences, shape (batch_size, target_sequence_length).\n",
    "        enc_hidden (tf.Tensor): Initial hidden state of the encoder, shape (batch_size, encoder_units).\n",
    "\n",
    "    Returns:\n",
    "        batch_loss (tf.Tensor): Average loss over the batch.\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass through the encoder\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        # Initialize decoder hidden state from encoder's last hidden state\n",
    "        dec_hidden = [enc_hidden[0][:, :units], enc_hidden[1][:, :units]]\n",
    "\n",
    "        # Initialize decoder input with start token\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index[\"<start>\"]] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # Forward pass through the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            # Compute loss\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # Use teacher forcing by passing the target as the next input\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    # Compute average loss over the batch\n",
    "    batch_loss = loss / int(targ.shape[1])\n",
    "\n",
    "    # Get trainable variables\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "lo-wQ2NPXOKm"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def validation_step(\n",
    "    inp: tf.Tensor, targ: tf.Tensor, enc_hidden: tf.Tensor\n",
    ") -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Performs a single validation step.\n",
    "\n",
    "    Args:\n",
    "        inp (tf.Tensor): Input sequences, shape (batch_size, input_sequence_length).\n",
    "        targ (tf.Tensor): Target sequences, shape (batch_size, target_sequence_length).\n",
    "        enc_hidden (tf.Tensor): Initial hidden state of the encoder, shape (batch_size, encoder_units).\n",
    "\n",
    "    Returns:\n",
    "        val_loss (tf.Tensor): Average validation loss over the batch.\n",
    "    \"\"\"\n",
    "    val_loss = 0\n",
    "    val_samples = 0\n",
    "\n",
    "    # Forward pass through the encoder\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    # Initialize decoder hidden state from encoder's last hidden state\n",
    "    dec_hidden = [enc_hidden[0][:, :units], enc_hidden[1][:, :units]]\n",
    "\n",
    "    # Initialize decoder input with start token\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index[\"<start>\"]] * BATCH_SIZE, 1)\n",
    "\n",
    "    for t in range(1, targ.shape[1]):\n",
    "        # Forward pass through the decoder\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(targ[:, t], predictions)\n",
    "        val_loss += loss\n",
    "        val_samples += 1\n",
    "\n",
    "        # Use teacher forcing by passing the target as the next input\n",
    "        dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    # Compute average validation loss over the batch\n",
    "    val_loss /= val_samples\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWKFyJ9ey__t"
   },
   "source": [
    "###Training the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ey5b9SESu7Wi",
    "outputId": "13aa2d63-5566-433a-ae1b-911c0383afc0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 52/52 [01:21<00:00,  1.58s/batch, loss=tf.Tensor(2.0429952, shape=(), dtype=float32)]\n",
      "Epoch 2: 100%|██████████| 52/52 [00:40<00:00,  1.27batch/s, loss=tf.Tensor(1.7405486, shape=(), dtype=float32)]\n",
      "Epoch 3: 100%|██████████| 52/52 [00:22<00:00,  2.34batch/s, loss=tf.Tensor(1.5771211, shape=(), dtype=float32)]\n",
      "Epoch 4: 100%|██████████| 52/52 [00:40<00:00,  1.27batch/s, loss=tf.Tensor(1.4425851, shape=(), dtype=float32)]\n",
      "Epoch 5: 100%|██████████| 52/52 [00:21<00:00,  2.39batch/s, loss=tf.Tensor(1.3388522, shape=(), dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Define the number of epochs\n",
    "EPOCHS = 5\n",
    "\n",
    "# Lists to store training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    enc_hidden = (\n",
    "        encoder.initialize_hidden_state()\n",
    "    )  # Initialize encoder hidden state for each epoch\n",
    "    total_loss = 0\n",
    "\n",
    "    # Create a tqdm progress bar for training batches\n",
    "    with tqdm(total=steps_per_epoch, desc=f\"Epoch {epoch}\", unit=\"batch\") as pbar:\n",
    "        for batch, (inp, targ) in enumerate(dataset.take(steps_per_epoch)):\n",
    "            batch_loss = train_step(inp, targ, enc_hidden)\n",
    "            total_loss += batch_loss\n",
    "            pbar.update(1)  # Update progress bar\n",
    "            pbar.set_postfix(\n",
    "                {\"loss\": total_loss / (batch + 1)}\n",
    "            )  # Update loss in progress bar\n",
    "\n",
    "    # Validation loop over batches\n",
    "    validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (input_tensor_val, target_tensor_val)\n",
    "    )\n",
    "    validation_dataset = validation_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    val_loss = 0\n",
    "    val_samples = 0\n",
    "    for batch, (inp, targ) in enumerate(validation_dataset):\n",
    "        enc_hidden = (\n",
    "            encoder.initialize_hidden_state()\n",
    "        )  # Initialize encoder hidden state for each batch\n",
    "        val_batch_loss = validation_step(inp, targ, enc_hidden)\n",
    "        val_loss += val_batch_loss\n",
    "        val_samples += 1\n",
    "\n",
    "    # Compute average validation loss\n",
    "    val_loss /= val_samples\n",
    "\n",
    "    # Store training and validation losses for plotting\n",
    "    train_losses.append(total_loss / steps_per_epoch)\n",
    "    val_losses.append(val_loss)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
